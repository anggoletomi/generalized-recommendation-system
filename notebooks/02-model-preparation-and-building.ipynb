{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from surprise import Dataset, Reader, accuracy\n",
    "from surprise.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from surprise import SVD, SVDpp\n",
    "from collections import defaultdict\n",
    "import joblib\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset. Shape: (383842, 14)\n",
      "After filtering: (145890, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>book_rating</th>\n",
       "      <th>user_age</th>\n",
       "      <th>user_city</th>\n",
       "      <th>user_state</th>\n",
       "      <th>user_country</th>\n",
       "      <th>book_title</th>\n",
       "      <th>book_author</th>\n",
       "      <th>book_year_of_publication</th>\n",
       "      <th>book_publisher</th>\n",
       "      <th>book_image_url_s</th>\n",
       "      <th>book_image_url_m</th>\n",
       "      <th>book_image_url_l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>276747</td>\n",
       "      <td>0060517794</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>IOWA CITY</td>\n",
       "      <td>IOWA</td>\n",
       "      <td>USA</td>\n",
       "      <td>Little Altars Everywhere</td>\n",
       "      <td>Rebecca Wells</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>HarperTorch</td>\n",
       "      <td>http://images.amazon.com/images/P/0060517794.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060517794.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060517794.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>276747</td>\n",
       "      <td>0671537458</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>IOWA CITY</td>\n",
       "      <td>IOWA</td>\n",
       "      <td>USA</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Terry McMillan</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>Pocket</td>\n",
       "      <td>http://images.amazon.com/images/P/0671537458.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0671537458.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0671537458.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>276747</td>\n",
       "      <td>0679776818</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>IOWA CITY</td>\n",
       "      <td>IOWA</td>\n",
       "      <td>USA</td>\n",
       "      <td>Birdsong: A Novel of Love and War</td>\n",
       "      <td>Sebastian Faulks</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>Vintage Books USA</td>\n",
       "      <td>http://images.amazon.com/images/P/0679776818.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0679776818.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0679776818.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>276813</td>\n",
       "      <td>8426449476</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>SITGES</td>\n",
       "      <td>BARCELONA</td>\n",
       "      <td>SPAIN</td>\n",
       "      <td>El Diaro De Bridget Jones</td>\n",
       "      <td>Helen Fielding</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>Lumen Espana</td>\n",
       "      <td>http://images.amazon.com/images/P/8426449476.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/8426449476.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/8426449476.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>276822</td>\n",
       "      <td>0060096195</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>CALGARY</td>\n",
       "      <td>ALBERTA</td>\n",
       "      <td>CANADA</td>\n",
       "      <td>The Boy Next Door</td>\n",
       "      <td>Meggin Cabot</td>\n",
       "      <td>2002.0</td>\n",
       "      <td>Avon Trade</td>\n",
       "      <td>http://images.amazon.com/images/P/0060096195.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060096195.0...</td>\n",
       "      <td>http://images.amazon.com/images/P/0060096195.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id        isbn  book_rating  user_age  user_city user_state  \\\n",
       "4    276747  0060517794            9        25  IOWA CITY       IOWA   \n",
       "5    276747  0671537458            9        25  IOWA CITY       IOWA   \n",
       "6    276747  0679776818            8        25  IOWA CITY       IOWA   \n",
       "33   276813  8426449476            8        29     SITGES  BARCELONA   \n",
       "39   276822  0060096195           10        11    CALGARY    ALBERTA   \n",
       "\n",
       "   user_country                         book_title       book_author  \\\n",
       "4           USA           Little Altars Everywhere     Rebecca Wells   \n",
       "5           USA                  Waiting to Exhale    Terry McMillan   \n",
       "6           USA  Birdsong: A Novel of Love and War  Sebastian Faulks   \n",
       "33        SPAIN          El Diaro De Bridget Jones    Helen Fielding   \n",
       "39       CANADA                  The Boy Next Door      Meggin Cabot   \n",
       "\n",
       "    book_year_of_publication     book_publisher  \\\n",
       "4                     2003.0        HarperTorch   \n",
       "5                     1995.0             Pocket   \n",
       "6                     1997.0  Vintage Books USA   \n",
       "33                    1996.0       Lumen Espana   \n",
       "39                    2002.0         Avon Trade   \n",
       "\n",
       "                                     book_image_url_s  \\\n",
       "4   http://images.amazon.com/images/P/0060517794.0...   \n",
       "5   http://images.amazon.com/images/P/0671537458.0...   \n",
       "6   http://images.amazon.com/images/P/0679776818.0...   \n",
       "33  http://images.amazon.com/images/P/8426449476.0...   \n",
       "39  http://images.amazon.com/images/P/0060096195.0...   \n",
       "\n",
       "                                     book_image_url_m  \\\n",
       "4   http://images.amazon.com/images/P/0060517794.0...   \n",
       "5   http://images.amazon.com/images/P/0671537458.0...   \n",
       "6   http://images.amazon.com/images/P/0679776818.0...   \n",
       "33  http://images.amazon.com/images/P/8426449476.0...   \n",
       "39  http://images.amazon.com/images/P/0060096195.0...   \n",
       "\n",
       "                                     book_image_url_l  \n",
       "4   http://images.amazon.com/images/P/0060517794.0...  \n",
       "5   http://images.amazon.com/images/P/0671537458.0...  \n",
       "6   http://images.amazon.com/images/P/0679776818.0...  \n",
       "33  http://images.amazon.com/images/P/8426449476.0...  \n",
       "39  http://images.amazon.com/images/P/0060096195.0...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Load & Filter\n",
    "\n",
    "processed_data_path = \"../data/processed/ratings_with_detailed_users_and_books.csv\"\n",
    "\n",
    "if not os.path.exists(processed_data_path):\n",
    "    raise FileNotFoundError(f\"File not found: {processed_data_path}\")\n",
    "\n",
    "df = pd.read_csv(processed_data_path)\n",
    "print(f\"Loaded dataset. Shape: {df.shape}\")\n",
    "\n",
    "# Define thresholds: any user and item must have at least 5 ratings\n",
    "# Users with fewer than 5 ratings provide very little information about their preferences, making them hard to model\n",
    "# Similarly, items with fewer than 5 ratings are difficult to compare against others\n",
    "min_ratings_user = 5\n",
    "min_ratings_item = 5\n",
    "\n",
    "user_counts = df.groupby('user_id')['isbn'].count()\n",
    "item_counts = df.groupby('isbn')['user_id'].count()\n",
    "\n",
    "valid_users = user_counts[user_counts >= min_ratings_user].index\n",
    "valid_items = item_counts[item_counts >= min_ratings_item].index\n",
    "\n",
    "df_filtered = df[df['user_id'].isin(valid_users) & df['isbn'].isin(valid_items)]\n",
    "print(f\"After filtering: {df_filtered.shape}\")\n",
    "\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready for Surprise.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Convert to Surprise\n",
    "\n",
    "reader = Reader(rating_scale=(1, 10)) # Tells Surprise that our ratings range from 1 (lowest) to 10 (highest)\n",
    "df_for_surprise = df_filtered[['user_id', 'isbn', 'book_rating']].copy()\n",
    "df_for_surprise.columns = ['userID', 'itemID', 'rating']\n",
    "\n",
    "data = Dataset.load_from_df(df_for_surprise, reader)\n",
    "print(\"Data ready for Surprise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Surprise is a Python toolkit for building and evaluating recommendation algorithms using (User, Item, Rating) data.\n",
    "- Surprise uses its own trainset/testset format under the hood. We can’t just pass a DataFrame directly to Surprise, we have to do this conversion.\n",
    "- By specifying the rating_scale, the library knows the range of valid ratings. It helps Surprise interpret how far off a prediction is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset users: 12097, items: 13724, ratings: 116712\n",
      "Testset size: 29178\n",
      "RMSE: 1.5781\n",
      "Baseline SVD RMSE: 1.5781\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Baseline SVD\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=0.2, random_state=42)\n",
    "print(f\"Trainset users: {trainset.n_users}, items: {trainset.n_items}, ratings: {trainset.n_ratings}\")\n",
    "print(f\"Testset size: {len(testset)}\")\n",
    "\n",
    "svd_baseline = SVD(n_factors=50, random_state=42)\n",
    "svd_baseline.fit(trainset)\n",
    "\n",
    "predictions = svd_baseline.test(testset)\n",
    "rmse_baseline = accuracy.rmse(predictions, verbose=True)\n",
    "print(f\"Baseline SVD RMSE: {rmse_baseline:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **SVD (Singular Value Decomposition) is** a mathematical method that can uncovers hidden patterns in user–book rating data\n",
    "- **The reason we use SVD is** that it often yields better predictions with minimal inputs: just user, item, and rating\n",
    "- **We define n_factors=50** to let the model capture 50 different \"preference dimensions\" for users and books\n",
    "- **From the result, we get RMSE ≈ 1.57**, meaning the model is off by about 1.57 points on a 1–10 scale. Example: if a true rating is 8, it might guess around 6.4 or 9.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 3 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Mean    Std     \n",
      "RMSE (testset)    1.5738  1.5884  1.5958  1.5860  0.0091  \n",
      "MAE (testset)     1.2165  1.2258  1.2291  1.2238  0.0053  \n",
      "Fit time          0.90    0.88    1.00    0.92    0.05    \n",
      "Test time         0.19    0.44    0.35    0.33    0.10    \n",
      "3-Fold CV: RMSE=1.5860, MAE=1.2238\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Cross-Validation (Baseline)\n",
    "\n",
    "cv_results = cross_validate(svd_baseline, data, measures=['RMSE', 'MAE'], cv=3, verbose=True)\n",
    "avg_rmse = np.mean(cv_results['test_rmse'])\n",
    "avg_mae = np.mean(cv_results['test_mae'])\n",
    "print(f\"3-Fold CV: RMSE={avg_rmse:.4f}, MAE={avg_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are doing a **Cross-validation** to split our data into multiple \"folds\" and repeatedly train/test the model on different splits.  \n",
    "- **The reason we do cross-validation** is to get a more reliable measure of performance than a single train–test split can provide.  \n",
    "- **We use 3 folds** so each fold acts as its own test set once, then we average results.  \n",
    "- **From the output**, we see an **RMSE of about 1.5872** and **MAE of about 1.2233**, indicating how far our predictions deviate from real ratings on average (RMSE) and how large the typical absolute error is (MAE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch for SVD (moderate grid)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 out of  72 | elapsed:   12.7s remaining:    1.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD GridSearch done.\n",
      "\n",
      "Best RMSE (SVD): 1.5777325908336586\n",
      "Best Params (SVD): {'n_factors': 10, 'reg_all': 0.1, 'lr_all': 0.01, 'n_epochs': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:   13.7s finished\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Hyperparameter Tuning (SVD)\n",
    "\n",
    "param_grid_svd = {\n",
    "    'n_factors': [10, 20, 50],\n",
    "    'reg_all': [0.02, 0.1],\n",
    "    'lr_all': [0.005, 0.01],\n",
    "    'n_epochs': [10, 20] \n",
    "}\n",
    "\n",
    "gs_svd = GridSearchCV(\n",
    "    SVD,\n",
    "    param_grid_svd,\n",
    "    measures=['rmse', 'mae'],\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    joblib_verbose=2\n",
    ")\n",
    "\n",
    "print(\"GridSearch for SVD (moderate grid)...\")\n",
    "gs_svd.fit(data)\n",
    "print(\"SVD GridSearch done.\\n\")\n",
    "\n",
    "print(\"Best RMSE (SVD):\", gs_svd.best_score['rmse'])\n",
    "print(\"Best Params (SVD):\", gs_svd.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are doing **Hyperparameter Tuning for SVD** to trying different values (like `n_factors` or `n_epochs`) to find which combination yields the best performance.  \n",
    "- **The reason we define a parameter grid** is so `GridSearchCV` can systematically test multiple settings such as `n_factors=[10, 20, 50]`, then tell us which ones minimize error.  \n",
    "- **We found the best parameters** to be `'n_factors': 10, 'reg_all': 0.1, 'lr_all': 0.01, 'n_epochs': 20`.  \n",
    "- **From the result, we get a best RMSE = 1.5789**, meaning on average our SVD model (with those parameters) is off by about 1.58 points on a 1–10 rating scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.5712\n",
      "SVD(Best) RMSE: 1.5712\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Train & Evaluate Best SVD\n",
    "\n",
    "best_params_svd = gs_svd.best_params['rmse']\n",
    "svd_best = SVD(\n",
    "    n_factors=best_params_svd['n_factors'],\n",
    "    reg_all=best_params_svd['reg_all'],\n",
    "    lr_all=best_params_svd['lr_all'],\n",
    "    n_epochs=best_params_svd['n_epochs'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Retrain with best hyperparams\n",
    "trainset_best_svd, testset_best_svd = train_test_split(data, test_size=0.2, random_state=42)\n",
    "svd_best.fit(trainset_best_svd)\n",
    "\n",
    "preds_best_svd = svd_best.test(testset_best_svd)\n",
    "rmse_best_svd = accuracy.rmse(preds_best_svd, verbose=True)\n",
    "print(f\"SVD(Best) RMSE: {rmse_best_svd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the best parameters found in the previous SVD grid search and retrain to confirm how well those parameters work when fully trained on the new split.  \n",
    "- **From the result, RMSE ≈ 1.5712**, which means it's off by about 1.57 points on a 1–10 rating scale, slightly better than the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearch for SVD++ (small grid)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 22 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  24 | elapsed:   34.3s remaining:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  24 | elapsed:   48.7s remaining:    9.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD++ GridSearch done.\n",
      "\n",
      "Best RMSE (SVD++): 1.5815827521457855\n",
      "Best Params (SVD++): {'n_factors': 10, 'reg_all': 0.1, 'lr_all': 0.005, 'n_epochs': 20}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:   56.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Hyperparameter Tuning (SVD++)\n",
    "\n",
    "param_grid_svdpp = {\n",
    "    'n_factors': [10, 20],\n",
    "    'reg_all': [0.02, 0.1],\n",
    "    'lr_all': [0.005],\n",
    "    'n_epochs': [10, 20]\n",
    "}\n",
    "\n",
    "gs_svdpp = GridSearchCV(\n",
    "    SVDpp,\n",
    "    param_grid_svdpp,\n",
    "    measures=['rmse', 'mae'],\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    joblib_verbose=2\n",
    ")\n",
    "\n",
    "print(\"GridSearch for SVD++ (small grid)...\")\n",
    "gs_svdpp.fit(data)\n",
    "print(\"SVD++ GridSearch done.\\n\")\n",
    "\n",
    "print(\"Best RMSE (SVD++):\", gs_svdpp.best_score['rmse'])\n",
    "print(\"Best Params (SVD++):\", gs_svdpp.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **SVD++ is** an advanced version of SVD that also leverages implicit feedback such as which books a user interacted with, even without explicit ratings\n",
    "- **We use a smaller parameter grid** than for regular SVD, since SVD++ can be more computationally heavy.  \n",
    "- **The best RMSE found** is about **1.5826**, with parameters `{'n_factors': 10, 'reg_all': 0.1, 'lr_all': 0.005, 'n_epochs': 20}`.  \n",
    "- **From this result**, SVD++ is slightly above our best SVD model's RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.5693\n",
      "SVD++(Best) RMSE: 1.5693\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Train & Evaluate Best SVD++\n",
    "\n",
    "best_params_svdpp = gs_svdpp.best_params['rmse']\n",
    "svdpp_best = SVDpp(\n",
    "    n_factors=best_params_svdpp['n_factors'],\n",
    "    reg_all=best_params_svdpp['reg_all'],\n",
    "    lr_all=best_params_svdpp['lr_all'],\n",
    "    n_epochs=best_params_svdpp['n_epochs'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Retrain with best hyperparams\n",
    "trainset_best_svdpp, testset_best_svdpp = train_test_split(data, test_size=0.2, random_state=42)\n",
    "svdpp_best.fit(trainset_best_svdpp)\n",
    "\n",
    "preds_best_svdpp = svdpp_best.test(testset_best_svdpp)\n",
    "rmse_best_svdpp = accuracy.rmse(preds_best_svdpp, verbose=True)\n",
    "print(f\"SVD++(Best) RMSE: {rmse_best_svdpp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use the best parameters found in the previous SVD++ grid search and retrain to confirm how well those parameters work when fully trained on the new split.  \n",
    "- **From the result, RMSE ≈ 1.5693**, which means it's off by about 1.57 points on a 1–10 rating scale, slightly better than the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Model Comparison -----\n",
      "SVD  RMSE: 1.5712, Precision@5: 0.377, Recall@5: 0.815\n",
      "SVD++ RMSE: 1.5693, Precision@5: 0.378, Recall@5: 0.816\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Compare Models (Ranking)\n",
    "\n",
    "def precision_recall_at_k(predictions, k=5, threshold=7):\n",
    "    user_est_true = defaultdict(list)\n",
    "    for uid, _, true_r, est, _ in predictions:\n",
    "        user_est_true[uid].append((est, true_r))\n",
    "\n",
    "    precisions, recalls = {}, {}\n",
    "    for uid, ratings in user_est_true.items():\n",
    "        ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "        top_k = ratings[:k]\n",
    "        n_rel = sum(true_r >= threshold for (_, true_r) in ratings)\n",
    "        n_rec_k = sum(true_r >= threshold for (_, true_r) in top_k)\n",
    "\n",
    "        precisions[uid] = n_rec_k / k\n",
    "        recalls[uid] = n_rec_k / n_rel if n_rel else 0\n",
    "\n",
    "    avg_prec = sum(precisions.values()) / len(precisions)\n",
    "    avg_rec = sum(recalls.values()) / len(recalls)\n",
    "    return avg_prec, avg_rec\n",
    "\n",
    "prec_svd, rec_svd = precision_recall_at_k(preds_best_svd, k=5, threshold=7)\n",
    "prec_svdpp, rec_svdpp = precision_recall_at_k(preds_best_svdpp, k=5, threshold=7)\n",
    "\n",
    "print(\"----- Model Comparison -----\")\n",
    "print(f\"SVD  RMSE: {rmse_best_svd:.4f}, Precision@5: {prec_svd:.3f}, Recall@5: {rec_svd:.3f}\")\n",
    "print(f\"SVD++ RMSE: {rmse_best_svdpp:.4f}, Precision@5: {prec_svdpp:.3f}, Recall@5: {rec_svdpp:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **We define relevant items as those with a true rating ≥ 7.** This lets us count how many \"good\" items show up in the top 5 recommendations.\n",
    "\n",
    "- **Precision@5** measures the fraction of those 5 recommended items that are actually relevant. For example, if 2 of the recommended books had a rating ≥7, then Precision@5 = 2/5 = 0.4.\n",
    "\n",
    "- **Recall@5** measures how many of all the user's relevant items we actually caught in our top 5. If a user has 5 relevant books in total, and our top-5 list contains 4 of them, Recall@5 = 4/5 = 0.8.\n",
    "\n",
    "- **SVD** gives RMSE=1.5712, Precision@5=0.377, and Recall@5=0.815. That means:  \n",
    "  - On a 1–10 scale, we're off by about 1.57 points in rating predictions, on average.  \n",
    "  - Out of the top 5 items recommended, 37.7% are truly relevant (≥7 rating), covering 81.5% of all relevant items.\n",
    "\n",
    "- **SVD++** is slightly better, with RMSE=1.5693, Precision@5=0.378, and Recall@5=0.816, showing marginally higher rating accuracy and capturing a slightly higher fraction of relevant items in its top 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 for user 276747 with chosen model:\n",
      "  ISBN: 0375502971, Score=0.00, Title=A Dog Year: Twelve Months, Four Dogs, and Me\n",
      "  ISBN: 3257204981, Score=0.00, Title=Der Vater Eines Morders\n",
      "  ISBN: 089471838X, Score=0.00, Title=Natural California: A Postcard Book\n",
      "  ISBN: 0689851324, Score=0.00, Title=Homecoming\n",
      "  ISBN: 0312253737, Score=0.00, Title=The Basic Eight\n",
      "Final model saved to ../models/recommender_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Recommendation & Save Model\n",
    "\n",
    "def recommend_books_for_user(\n",
    "    model,\n",
    "    df_full,\n",
    "    user_id,\n",
    "    top_n=5,\n",
    "    exclude_rated=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Recommend top_n books for a given user using a trained Surprise model.\n",
    "    If the user is unknown, do a fallback based on global popularity.\n",
    "\n",
    "    :param model:       Trained Surprise model.\n",
    "    :param df_full:     The Pandas DataFrame with columns [user_id, isbn, book_rating, book_title...].\n",
    "    :param user_id:     The user to recommend items for.\n",
    "    :param top_n:       Number of recommendations to return.\n",
    "    :param exclude_rated: If True, excludes items the user has already rated.\n",
    "    :return: A list of tuples: (isbn, predicted_score, title).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model.trainset.to_inner_uid(str(user_id))\n",
    "\n",
    "        all_isbns = df_full['isbn'].unique()\n",
    "\n",
    "        if exclude_rated:\n",
    "            already_rated = df_full[df_full['user_id'] == user_id]['isbn'].unique()\n",
    "            candidate_isbns = [isbn for isbn in all_isbns if isbn not in already_rated]\n",
    "        else:\n",
    "            candidate_isbns = all_isbns\n",
    "\n",
    "        # 2) Predict for candidate items only\n",
    "        predictions = []\n",
    "        for isbn in candidate_isbns:\n",
    "            pred = model.predict(str(user_id), str(isbn), verbose=False)\n",
    "            predictions.append((isbn, pred.est))\n",
    "\n",
    "        # 3) Sort descending by predicted rating\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_preds = predictions[:top_n]\n",
    "\n",
    "        # 4) Map ISBN -> Title\n",
    "        isbn_to_title = df_full.drop_duplicates('isbn').set_index('isbn')['book_title'].to_dict()\n",
    "        results = [(isbn, score, isbn_to_title.get(isbn, \"Unknown Title\")) for isbn, score in top_preds]\n",
    "        return results\n",
    "\n",
    "    except ValueError:\n",
    "        top_global = (\n",
    "            df_full.groupby('isbn')['book_rating']\n",
    "            .mean()\n",
    "            .sort_values(ascending=False)\n",
    "            .head(top_n)\n",
    "            .index.tolist()\n",
    "        )\n",
    "        isbn_to_title = df_full.drop_duplicates('isbn').set_index('isbn')['book_title'].to_dict()\n",
    "        return [(isbn, 0.0, isbn_to_title.get(isbn, \"Unknown Title\")) for isbn in top_global]\n",
    "\n",
    "# Choose best model from ranking or RMSE\n",
    "model_choice = svd_best  # or svdpp_best\n",
    "\n",
    "user_example = df_filtered['user_id'].iloc[0]\n",
    "top_n = 5\n",
    "top_recs = recommend_books_for_user(model_choice, df_filtered, user_example, top_n=top_n)\n",
    "\n",
    "print(f\"Top {top_n} for user {user_example} with chosen model:\")\n",
    "for isbn, score, title in top_recs:\n",
    "    print(f\"  ISBN: {isbn}, Score={score:.2f}, Title={title}\")\n",
    "\n",
    "model_path = \"../models/recommender_model.joblib\"\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "joblib.dump(model_choice, model_path)\n",
    "print(f\"Final model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Function **`recommend_books_for_user`** takes a trained model and a user ID, then returns top-N recommended books.  \n",
    "- **2 scenarios in the function**: if the user is known, predict ratings for all items. If the user is unknown, fallback to globally popular books.  \n",
    "- **Why exclude_rated?** Sometimes we want to skip items the user already rated (in production mode), so they only see new suggestions.  \n",
    "- **In this example**, we pick `svd_best` as our final model and call `recommend_books_for_user` with `top_n=5`.  \n",
    "- **From the result**, the model recommends five ISBNs with a score of 0.0, which suggests it had limited info on that user–item combination (or the model didn't predict a higher rating). In a real scenario, we might see non-zero scores if there's more overlap in the training data.  \n",
    "- Model saved to `../models/recommender_model.joblib`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
